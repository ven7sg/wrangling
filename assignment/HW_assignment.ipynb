{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling\n",
        "# Tamera Fang (ven7sg)\n",
        "## `! git clone https://github.com/DS3001/wrangling`\n",
        "## Do Q2, and one of Q1 or Q3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  \n",
        "  The paper addresses an important part of data cleaning known as data tidying. The framework consists of making each variable a column, each observation a row, and each type of observational unit a table. This makes it easier and more effective to manipulate, visualize, and model datasets. Additionally, it helps in data analysis due to consistency in data structure and better developed tools to utilize.  \n",
        "\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "\n",
        "  The \"tidy data standard\" is intended to structure datasets in an organized manner to facilitate analysis and exploration. It makes the initial data cleaning process easier because of the organized set framework.\n",
        "\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  \n",
        "  The first sentence relating datasets to families means that tidy datasets are often consistent and easy to analyze due to the standard they have. On the other hand, a messy dataset or messy family can mean a variety of things, for each mess can be unique and lead to different problems that complicate the dataset or family dynamic. The second sentence intends to explain that while you can easily identify certain aspects within some individual datasets often due to intuition, generalizing it is often a difficult thing to do due to the range of research and elements analysts are exploring. It can become complicated to identify certain observations or variables depending on the goals and context of the data.    \n",
        "\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "\n",
        "  Wickham defines values as quantitative or qualitative elements that comprise a dataset. Each value is organized as either a variable or observation. A variable is a value that measures the same underlying attribute across units. Observations are values measured on the same unit across attributes.\n",
        "\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "\n",
        "  In section 2.3, \"Tidy Data\" is defined as a standard way of mapping the meaning of a dataset to its structure. In tidy data, each variable will form a column, each observation forms a row, and each type of observational unit forms a table.\n",
        "\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "\n",
        "  The 5 most common problems with messy datasets include column headers as values and not variable names, multiple variables being stored in one column, variables being stored in both rows and columns, mulitple types of observational units stored in the same table, and a single observational unit stored in multiple tables. The data in Table 4 is considered messy because we see the column headers as values and not variable names. In this example, the columns are different value ranges of income, in which income would actually be considered one variable here. The other variables are frequency and religion. Hence, in total there should actually be three seperate columns. \"Melting\" a dataset, or stacking it, is how we could tidy this up. This means we convert the columns into rows.   \n",
        "\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "\n",
        "  Table 11 is considered messy because the days as column headers are considered values rather than variable names. This is melted down in Table 12A which we call \"molten\" because now the days are down to a single variable column labeled as \"date\". However, we can't call this table tidy yet because instead of values, the element column contains names of variables. This is why in Table 12B, the variable names (tmax and tmin) that were in the previous element column are now columns on their own, ensuring that all columns are variables and all rows are observations. This makes Table 12B tidy.   \n",
        "\n",
        "\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
        "\n",
        "  The \"chicken-and-egg\" problem is if tidy data is only as useful as the tools that work with it, then tidy tools will be inextricably linked to tidy data. This causes the problem in which independently changing data structures or data tools will not improve workflow. In the future, Wickham has hopes to use methodologies from the fields of user-testing, ethnography, and talk-aloud protocols to improve understanding of the cognitive side of data analysis. He also hopes to further improve our ability to design appropriate tools."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "\n",
        "\n",
        "```\n",
        "# pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/Users/tamerafang/PycharmProjects/DataWrangling/venv/airbnb_hw.csv', low_memory=False)\n",
        "print( df.shape, '\\n')\n",
        "df.head()\n",
        "\n",
        "price = df['Price'] # extraction\n",
        "price = price.str.replace(',','') # replace commas\n",
        "print( price.unique() , '\\n') # check commas are removed\n",
        "price = pd.to_numeric(price,errors='coerce') # coversion\n",
        "print( price.unique() , '\\n')\n",
        "print( 'Total missing: ', sum( price.isnull() ) )\n",
        "```\n",
        "Explanation: We start by cleaning the Price column by extracting it so that we can focus on it. Then, we remove all the commas to make sure the formatting of all the prices are consistent. Using the unique() function, we're able to check all unique values. Lastly, we convert the column to a numeric data type. Together, these choices remove fix formatting inconsistencies for easier data analysis.\n",
        "\n",
        "Answer: Total missing values is 0.\n",
        "\n",
        "\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "\n",
        "\n",
        "```\n",
        "# pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/Users/tamerafang/PycharmProjects/DataWrangling/venv/sharks.csv', low_memory=False)\n",
        "\n",
        "df['Type'].value_counts() # count initial occurrences\n",
        "type = df['Type'] # create a temporary vector of values to look into\n",
        "type = type.replace(['Sea Disaster', 'Boat','Boating','Boatomg'],'Watercraft') # condense all the watercraft categories into one\n",
        "type = type.replace(['Invalid', 'Questionable','Unconfirmed','Unverified','Under investigation'],np.nan) # all uncertain values\n",
        "type.value_counts() # check counts before replacing versions\n",
        "\n",
        "df['Type'] = type # replace 'Type' variable with the clean version\n",
        "del type # get rid of the temporary vector\n",
        "\n",
        "df['Type'].value_counts() # final occurrence count in clean version\n",
        "```\n",
        "Explanation: We start off by creating a temporary vector to hold our clean elements. Then we start cleaning the \"type\" variable by condensing the watercraft-related incidents into a single \"watercraft\" category for simplicity. We take a similar approach for uncertain values as missing using the np.nan function. After consolidating these values and considering them as clean, we replace our previous messy type variable with the clean version.\n",
        "\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "\n",
        "```\n",
        "# pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/Users/tamerafang/PycharmProjects/DataWrangling/venv/VirginiaPretrialData2017.csv', low_memory=False)\n",
        "\n",
        "release = df['WhetherDefendantWasReleasedPretrial'] # create temporary vector\n",
        "release = release.replace(9,np.nan) # 9 is unclear so replace\n",
        "print(release.value_counts(),'\\n') # check counts for verification\n",
        "sum(release.isnull()) # 31 missing values\n",
        "df['WhetherDefendantWasReleasedPretrial'] = release # replace with cleaned values\n",
        "del release # delete temporary vector\n",
        "```\n",
        "Explanation: We create a temporary vector to store the clean elements. We replace the 9 values, which indicate unclear data, with np.nan to denote them as missing. After this, we can check the count to make sure these were correctly replaced. Then, we count or sum up the total number of missing values. Once verified, we replace the old messy vector with the finalized clean column. Lastly, we delete the temporary vector, for we have already replaced it.\n",
        "\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)\n",
        "\n",
        "```\n",
        "# pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/Users/tamerafang/PycharmProjects/DataWrangling/venv/VirginiaPretrialData2017.csv', low_memory=False)\n",
        "\n",
        "length = df['ImposedSentenceAllChargeInContactEvent'] # temp length, col UF\n",
        "type = df['SentenceTypeAllChargesAtConvictionInContactEvent'] # temp type, from hint, col UE\n",
        "\n",
        "length = pd.to_numeric(length,errors='coerce') # convert length to numeric and non-numeric as missing\n",
        "\n",
        "length_NA = length.isnull() # identify missing values\n",
        "print( np.sum(length_NA),'\\n') # check sum of missing\n",
        "print( pd.crosstab(length_NA, type), '\\n') # check length against type\n",
        "\n",
        "length = length.mask( type == 4, 0) # replace type 4 length to 0\n",
        "length = length.mask( type == 9, np.nan) # replace type 9 length with np.nan\n",
        "\n",
        "length_NA = length.isnull() # identify missing values again\n",
        "print( pd.crosstab(length_NA, type), '\\n') # check length against type\n",
        "print( np.sum(length_NA),'\\n') # check length sum of missing, there are fewer now\n",
        "\n",
        "df['ImposedSentenceAllChargeInContactEvent'] = length # replace data with clean version\n",
        "del length, type # delete temp length/type variables\n",
        "```\n",
        "Explanation: Like the previous questions, we extract and make temporary vectors for the columns we're cleaning, so in this case we have length and type. Next, we convert the sentence length data to numeric values and all the non-numeric ones to np.nan to represent it as missing. We then identify the missing values and sum it up to see where we're at in the data cleaning process. For visualization, we check missing values in length against type. Then, we replace type 4 lengths with 0, for they are dismissed charges. We also replace type 9 length with np.nan as they are considered missing. After cleaning this up, we check our missing values again for verification and notice that the missing elements have significantly gone down. Once clean, we replace the previous messy columns with our new clean columns, also ensuring to delete the old temporary ones in the end.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}